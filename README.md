# Classical Machine Learning

This repository captures the full suite of classical ML â€” including regression, classification, clustering, dimensionality reduction, and ensemble methods â€” implemented with rigorous preprocessing and evaluation.

---

## ðŸ§  Topics Covered

### ðŸ” Supervised Learning

#### Regression
- Linear Regression (OLS)
- Ridge / Lasso / ElasticNet
- Polynomial Regression
- Poisson & Gamma Generalized Linear Models (GLM)
- Spline Regression

ðŸ“Œ **Projects:**
---
### ðŸ”— Featured Full-Stack Project

#### ðŸ  [House Price Prediction (Full Stack)](https://github.com/Tamaghnatech/HousePricePrediction)

A **production-grade, end-to-end regression pipeline** implementing:

* **Linear Regression (OLS)**
* **Ridge Regression**
* **Lasso Regression**
* **ElasticNet Regression**

ðŸš€ Features:

* ðŸ“Š Cross-validated model comparisons
* ðŸª„ Auto-logging with **Weights & Biases (wandb)**
* ðŸ’¾ Model saving and artifact tracking
* ðŸŒ **Interactive Streamlit dashboard** with real-time metrics
* ðŸ“ Clean repo structure, ðŸ“‰ Matplotlib plots, and ðŸ“„ Beautiful README

> *This repo serves as a showcase of how classical linear models can be taken all the way to production-like deployment using modern tools.*

---
- CO2 Emissions vs Engine Size (Polynomial Fit)
- French Motor Insurance Claims (Poisson/Gamma GLM)

#### Classification
- Logistic Regression
- Naive Bayes
- k-Nearest Neighbors (k-NN)
- Linear Discriminant Analysis (LDA)
- SVM (Linear & RBF)

ðŸ“Œ **Projects:**
- SMS Spam Detection
- Credit Card Default Prediction
- Handwritten Digit Recognition (MNIST)
- Wine Type Classification

---

### ðŸŒ² Tree-Based Models
- Decision Tree / Regression Tree (CART)
- Random Forest
- Extra Trees
- Feature Importance & Visualization

ðŸ“Œ **Projects:**
- Titanic Survival Classification
- Heart Disease Prediction
- Diabetes Progression (Regression Tree)

---

### âš¡ Boosting Algorithms
- AdaBoost
- Gradient Boosting
- XGBoost, LightGBM, CatBoost

ðŸ“Œ **Projects:**
- Income Classification (>$50k)
- Loan Approval Prediction
- Gradient Boosting vs AdaBoost Comparison

---

### ðŸ“ˆ Model Evaluation & Selection
- Regression Metrics: MSE, RMSE, MAE, RÂ², MAPE
- Classification Metrics: Precision, Recall, F1, ROC-AUC, PR-AUC, MCC, Cohenâ€™s Kappa
- Ranking Metrics: nDCG, MAP
- Cross-validation: k-Fold, Stratified, Time-Series, Nested
- Hyperparameter Tuning: GridSearchCV, RandomizedSearchCV, Optuna

---

### ðŸ§¼ Preprocessing & Feature Engineering
- Handling Missing Data
- Scaling: Standard, MinMax, Robust
- Encoding: One-Hot, Target, Frequency
- Log Transform, Box-Cox
- Feature Generation (Polynomial, Interaction)
- Date/Time Feature Extraction
- Text Vectorization: TF-IDF, CountVectorizer

ðŸ“Œ **Datasets:**
- Adult Income Dataset
- Housing Prices (Kaggle)
- UCI Heart, Wine, and Credit Card datasets

---

### ðŸ¤– Unsupervised Learning

#### Clustering
- K-Means
- Hierarchical Clustering
- DBSCAN
- Gaussian Mixture Models (GMM)

ðŸ“Œ **Projects:**
- Customer Segmentation (Mall Data)
- Gene Expression Clustering
- Credit Card Fraud Detection (Anomaly DBSCAN)
- Speaker Identification with GMMs

#### Dimensionality Reduction
- PCA, t-SNE, UMAP
- Autoencoders (shallow)

ðŸ“Œ **Projects:**
- MNIST Visualization (t-SNE, PCA)
- Cancer Gene Mapping (UMAP)
- Image Compression with PCA

#### Association Rule Mining
- Apriori, Eclat, FP-Growth

ðŸ“Œ **Projects:**
- Market Basket Analysis (Groceries)
- Bookstore Purchase Pattern Mining
- Retail Rule Mining on Transactions

---

## ðŸ“ Folder Structure

- `notes/`  
  ðŸ“š Concepts, formulas, PDFs, cheat sheets.

- `projects/`  
  ðŸ’» Google Colab notebooks implementing supervised and unsupervised models end-to-end.

- `diagrams/`  
  ðŸŽ¨ Manim/draw.io visualizations of decision trees, clusters, PCA, bias-variance, etc.

---

## ðŸ› ï¸ Suggested Add-ons
- SHAP & LIME for model interpretability
- Pipelines with `sklearn.pipeline`
- Custom Transformers
- Model versioning using DVC

---

> Mastering classical ML gives you both intuition and control.  
> Deep learning begins â€” but never replaces â€” this foundation.
